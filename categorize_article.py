# -*- coding: utf-8 -*-
"""categorize_article.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10rNRcB1uhq4Y4erXWt4vZvBOATdWHtEG
"""

import textwrap
import numpy as np 
import pickle
import json
import nltk
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
import os
import datetime
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from tensorflow.keras.layers import SimpleRNN,Dense,LSTM,Dropout,Embedding,Bidirectional
from tensorflow.keras import Sequential, Input
from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.utils import plot_model
from keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau, ModelCheckpoint
from sklearn.model_selection import train_test_split
from keras_preprocessing.sequence import pad_sequences
from sklearn.preprocessing import OneHotEncoder
from typing import Text

# Download nltk stopwords for data cleaning

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('omw-1.4')

#%% 1. Data loading

CSV_URL = 'https://raw.githubusercontent.com/susanli2016/PyCon-Canada-2019-NLP-Tutorial/master/bbc-text.csv'

df = pd.read_csv(CSV_URL)

#%% 2. Data inspection

df.head(5)

df.tail(5)

df.info()
df.describe().T

df.isna().sum() # no NaN values

df.duplicated().sum() #Got 99 duplicates, need to be removed

df['text'][100] #Checking the text

# Plot of the category

plt.figure()
sns.displot(df['category'])
plt.show()

#%% 3. Data cleaning

#1) Remove numbers
#2) Remove HTML tags
#3) Remove punctuations
#4) Change all to lowercase()

#Remove duplicates
df = df.drop_duplicates()
df.duplicated().sum()

# Remove stopwords

STOPWORDS = set(stopwords.words('english'))

def clean_text(text, stop_words=STOPWORDS):
    lemmatizer = WordNetLemmatizer() #Lemmatize them words
    filter = '[^\w]'
    text = [lemmatizer.lemmatize(w) for w in word_tokenize(text.lower()) if w not in stop_words]
    text = ' '.join(text)
    return re.sub(filter, ' ', text)

df['text'] = df['text'].apply(clean_text)
df['text'][100]

# Remove numbers

def text_cleaning(text):
  text = re.sub('\d\w{0,10}','',text)

  return text

cleaned_text = []
for index, temp in enumerate(df['text']):
    cleaned_text.append(text_cleaning(temp))
df['text'] = cleaned_text

df['text'][100]

# Inspect for data in text column

np.sum(df['text'].str.split().str.len())
np.mean(df['text'].str.split().str.len())
np.median(df['text'].str.split().str.len())

#%% 4. Features selection
# Define the features and targets

features = df['text']
targets = df['category']

#%% 5. Data preprocessing

# Load tokenizer

num_words = 5000
oov_token = '<OOV>'  # out of vocab

tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)
tokenizer.fit_on_texts(features)
feature_sequences = tokenizer.texts_to_sequences(features)
word_index = tokenizer.word_index
print(dict(list(word_index.items())[0:10]))

# Padding & truncating

feature_sequences = pad_sequences(feature_sequences, maxlen=250, padding='post', truncating='post')
feature_sequences[598]

# Expand dimension

feature_sequences = np.expand_dims(feature_sequences, -1)
targets = np.expand_dims(targets, -1)

# One hot encoder

ohe = OneHotEncoder(sparse=False) # to change to list
targets = ohe.fit_transform(targets)

# Train test split

X_train,X_test,y_train,y_test = train_test_split(feature_sequences,targets,test_size=0.2,random_state=12345)

#%% 6. Model development

embedding_layer = 64

model = Sequential()
model.add(Embedding(num_words, embedding_layer))
model.add(Bidirectional(LSTM(embedding_layer)))
model.add(Dropout(0.3))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(y_train.shape[1], activation='softmax'))

# Model summary
model.summary()
plot_model(model, to_file=os.path.join(os.getcwd(), 'model.png'), show_shapes=True, show_layer_names=True)

# Model compile
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])

# Commented out IPython magic to ensure Python compatibility.
# Tensorboard callback 
# %load_ext tensorboard
LOG_DIR = os.path.join(os.getcwd(), 'logs', datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))

tb = TensorBoard(log_dir=LOG_DIR)
es = EarlyStopping(monitor='val_acc', patience=5, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5, min_lr=0.001)
mc = ModelCheckpoint(filepath=os.path.join(os.getcwd(), 'temp', 'checkpoint'), save_weights_only=True, 
                     monitor='val_acc', mode='max', save_best_only=True)

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs

# Model training

EPOCHS = 10
BATCH_SIZE = 64

hist = model.fit(X_train,y_train,validation_data=(X_test,y_test),batch_size=64, epochs=15,callbacks=[tb, es, reduce_lr, mc])

#%% 7. Model evaluation
# Prediction with the model
y_pred = np.argmax(model.predict(X_test), axis=1)
y_true = np.argmax(y_test, axis=1)

print('Classification report:\n', classification_report(y_true, y_pred))

# Model analysis

plt.figure()
plt.plot(hist.history['acc'])
plt.plot(hist.history['val_acc'])
plt.legend(['training','validation'])
plt.show()

#%% 8. Model saving

SAVE_PATH = os.path.join(os.getcwd(), 'saved_models')

model.save('model.h5')

#save ohe
with open('ohe.pkl','wb') as f:
  pickle.dump(ohe,f)

#tokenizer
with open('tokenizer.json','w') as f:
  json.dump(tokenizer.to_json(),f)